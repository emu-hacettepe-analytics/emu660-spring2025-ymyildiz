[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! This is Yiğit from EMU660 2024-2025 Spring Course.\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "",
    "text": "Every country has an inflation which might be either positive or negative. It is stated that positive inflation points to a country’s economic status regarding people’s power to buy and maintain their lives. For the last few years, Turkey has been one of the countries that have suffered from high inflation. Moreover, inflation has been increasing day by day. The inflation word consists of many sub-categories some of which are food, cloth, education, and transportation consumer price index (CPI). This project mainly focuses on the CPI of food and its effect on various groups of people with different total expenditure levels. To analyze the effect and infer, “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025” and “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” data are driven from the website of TUIK, which is short for Turkey Statistics Institute."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Inflation of Food and Its Affect on Different Expenditure Groups in Turkey",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nThe data used for the analysis is driven from TUIK website."
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.1 General Information About Data",
    "text": "2.1 General Information About Data\nThe data files used for in-depth analysis are driven from the TUIK website. The data named “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” consists of the distribution of consumption expenditure types of different expenditure groups. This data is also referred to as “comparison_of_consumption_types” and “COCT data” throughout the project. There are 5 groups in the columns named “First quantile”, “Second quantile”, “Third quantile”, “Fourth quantile”, and “Last quantile”. Each represents 20% of the people who are subject of the data research in an ascending order of expenditure amount. Namely, the First quantile represents the %20 of the people who spend the least while the Last quantile represents the people who spend the most. There are different expenditure types for the years in the rows. However, there is no data from the years 2020 and 2021 even though the data is named by 2002-2023. Besides, there are two types of expenditure categorization for 2022 with a slight difference in splitting one type into two types. This update of categorization continues in 2023 as well. Consequently, there are 12 different expenditure types for the years from 2002 to 2022, excluding 2020 and 2021, while there are 14 for the years 2022 and 2023.\n\nAnother data used is named “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025”, which is also referred to as “consumer_price_index_data” and “CPI data” throughout the project. This data shows the consumer price index value of 288 different main and sub-groups of expenditure according to each month of the years from 2005 to 2025. Year and month information is on the rows while the group names form the columns. Only the columns directly related to food are considered as the scope of this project.\n\nIn addition, the same years are considered to analyze the relation between CPI of food and its effect on different group of people with different levels of expenditure. Thus, the years between 2005-2023 for both of the data sets are selected as the interval of the project."
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.2 Reason of Choice",
    "text": "2.2 Reason of Choice\nThe inflation of a country tells a lot about its economic status. It tells so many things that the food aspect of it might be underestimated. Nevertheless, reaching food has been one of the major concerns of mankind. Over the decades, this concern has become more crucial for Turkish people, especially having low levels of income and belonging to the first and second quantile of expenditure level groups. The data chosen for this project helps us to understand which group of people spend what percentage of their money on food and how this rate changes according to the inflation and consumer index of food. The conclusion of the project might shed light on the facts like a sign of socioeconomic differences within the Turkish people. It might also enable society to comprehend the situation of access to quality food according to different expenditure groups."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.3 Preprocessing",
    "text": "2.3 Preprocessing\nTo begin with, the raw data which are in Excel (.xlsx) format driven from the website of TUIK are browsed as they are. Some of the rows and columns are deleted since they include texts providing information about the data. Later, Turkish headings are removed from each row and column in the files. After a few operations in the Excel format of the files, they become ready to be imported to R. Both data are also saved in RData format to be processed in R.\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row and 8 columns.\n\nsave(consumer_price_index_data, file = \"consumer_price_index_data.RData\")\nsave(comparison_of_consumption_types, file = \"comparison_of_consumption_types.RData\")\nload(\"consumer_price_index_data.RData\")\nload(\"comparison_of_consumption_types.RData\")\n\n\nnrow_CPI &lt;- nrow(consumer_price_index_data)\nncol_CPI &lt;- ncol(consumer_price_index_data)\nnrow_COCT &lt;- nrow(comparison_of_consumption_types)\nncol_COCT &lt;- ncol(comparison_of_consumption_types)\nsummary_of_number_of_row_and_columns &lt;- data.frame(\ndata_set_name = c(\"consumer_price_index_data\",\"comparison_of_consumption_types\"),\nnumber_of_rows = c(nrow_CPI, nrow_COCT),\nnumber_of_column = c(ncol_CPI, ncol_COCT)\n)\nsummary_of_number_of_row_and_columns\n\n                    data_set_name number_of_rows number_of_column\n1       consumer_price_index_data            243              291\n2 comparison_of_consumption_types            275                8\n\nhead(consumer_price_index_data)\n\n# A tibble: 6 × 291\n   Year Months   General Food and non-alcoholic beverag…¹ Alcoholic beverages …²\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;\n1  2005 January     114.                             112.                   122.\n2  2005 February    115.                             113.                   124.\n3  2005 March       115.                             113.                   125.\n4  2005 April       116.                             112.                   125.\n5  2005 May         117.                             112.                   125.\n6  2005 June        117.                             110.                   125.\n# ℹ abbreviated names: ¹​`Food and non-alcoholic beverages`,\n#   ²​`Alcoholic beverages and tobacco`\n# ℹ 286 more variables: `Clothing and footwear` &lt;dbl&gt;,\n#   `Housing, water, electricity, gas and other fuels` &lt;dbl&gt;,\n#   `Furnishings, household equipment, routine maintenance of the house` &lt;dbl&gt;,\n#   Health &lt;dbl&gt;, Transport &lt;dbl&gt;, Communications &lt;dbl&gt;,\n#   `Recreation and culture` &lt;dbl&gt;, Education &lt;dbl&gt;, …\n\nhead(comparison_of_consumption_types)\n\n# A tibble: 6 × 8\n  Year  `Expenditure Types`             Total `First quintile` `Second quintile`\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n1 2002  Total consumption expenditure  100              100               100   \n2 2002  Food and non-alcoholic bevera…  26.7             41.4              37.6 \n3 2002  Alcoholic beverages, cigaratt…   4.06             5.16              4.96\n4 2002  Clothing and footwear            6.27             3.34              4.65\n5 2002  Housing and rent                27.3             33.4              30.7 \n6 2002  Furniture, houses appliances …   7.29             2.45              3.02\n# ℹ 3 more variables: `Third quintile` &lt;dbl&gt;, `Fourth quintile` &lt;dbl&gt;,\n#   `Last quintile` &lt;dbl&gt;\n\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row ans 8 columns."
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.4 Results",
    "text": "3.4 Results"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignments I am and will be conducting for the Spring 2024-2025 EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nASELSAN A.S, Project Engineer, Sep 2022- (full-time)\nARCELIK A.S, Project Assistant, Jan-Apr 2022 (part-time)\nMercedes-Benz Turk A.S, PEP (Professional Experience Program), Jul-Dec 2021 (part-time)\nBosch Thermotechnic, Logistics Intern, Jun 2021 (internship)\nATM Beyaz Esya Parçalari San. ve Tic. Ltd., Operator, Jun-Aug 2020 (full-time)\nFerhanyildiz Rent a Car, Concierge-Driver, Jun-Aug 2019 (full-time)"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm aaa, position xx, year xxx\nFirm bbb, position yyy, year yyy"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This page consists of the final requirements of Assignment 1. There are three sub-parts of the 3rd task of the assignment, where first two tasks mainly focus on customization of the webpage and publishing the CV.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Education\n\nM.S., Industrial Engineering, Hacettepe University, Turkey, 2024-\nB.S., Industrial Engineering, Ihsan Dogramaci Bilkent University, Turkey, 2017 - 2022\n\n\n\nWork Experience\n\nASELSAN A.S, Project Engineer, Sep 2022- (full-time)\nARCELIK A.S, Project Assistant, Jan-Apr 2022 (part-time)\nMercedes-Benz Turk A.S, PEP (Professional Experience Program), Jul-Dec 2021 (part-time)\nBosch Thermotechnic, Logistics Intern, Jun 2021 (internship)\nATM Beyaz Esya Parçalari San. ve Tic. Ltd., Operator, Jun-Aug 2020 (full-time)\nFerhanyildiz Rent a Car, Concierge-Driver, Jun-Aug 2019 (full-time)\n\n\n\nProjects\n\nDecision Support System for Configuration Management of FNSS (Senior Year Project)\n\n\n\nPublications\n\nProgressing…\n\n\n\nCompetencies\nR, Quarto, Git, Python, Xpress, Primavera P6\n\n\nHobbies\nWorking out, watching-analysing movies, travelling\nCV\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-1.html#a",
    "href": "assignments/assignment-1.html#a",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\n\nA Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”\nThe talk is mainly about data science and its relation to industrial engineering according to Mr. Cem Vardar. The talk starts with a brief Introduction where he introduces himself. Mr. Vardar has been an industrial engineer for more than 20 years. After his PhD at Arizona State University, he worked in various tech companies as data scientist and analyst in the USA.\nAs a second section of the talk, he mentioned the concept of Engineering and Problem Solving. According to Mr. Vardar, an engineer solves problem withing the systems by using science and mathematical applications. Here, industrial engineers play a role of problem solver not just of any system but of complex systems. He also emphasizes the importance of initiating the solution to such systems’ problems with a basic approach even though a complex solution will be needed in the end, which is very similar to idea of evolution in his opinion. In this part he states that he supports the phrase “If it works, don’t touch it!” as a response to a student’s question. Later, in Data Science and Industrial Engineering part, he gathers the approaches of data science into sub-groups. These groups mainly use data science as a tool to solve problems, which is a huge plus for a company to analyze and learn.\nIn the fourth section, Carvana and Data Analytics/Science, he explains what the departments related to data do and which tools they are using while doing that in the company named Carvana where he used to work and witnessed its growth thanks to these departments. After that, he mentions the Qualifications of Data Scientists in the business sector and what to do to improve the skills. He basically divides skill into two headings: soft skills and technical skills. Then, he sincerely tells his Recommendations for the ones who are willing to be a successful data scientist as an industrial engineer. In the end, he mentions Reading, Listening and Watching List including some videos and books related to the speech he gave.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n\nExploring Statistical Summaries with Custom Functions and Iteration Methods\n\ndata(mtcars)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nHere, we called “mtcars” dataset.\nThen, the function returning a named list containing the mean, median, variance, IQR, minimum, and maximum of the input would be as follows:\n\ncompute_stats &lt;- function(x){\n  if (!is.numeric(x)){\n    stop(\"Input is not numeric vector.\") #We make sure that the function takes numeric vector as input.\n  }\n  \n  statistics_of_data &lt;- list(\n  mean =  mean(x),\n  median =  median(x),\n  variance = var(x),\n  IQR = IQR(x),\n  min = min(x),\n  max =  max(x)\n  )\n  \n  statistics_of_data\n}\n\nNow, let us apply the function using a for loop:\n\nfor (column_name in names(mtcars)){\n  if(is.numeric(mtcars[[column_name]])){\n    computed_statistics &lt;- compute_stats(mtcars[[column_name]])\n    cat(\"\\nStatistics for Column:\", column_name, \"\\n\")\n    print(computed_statistics)\n    cat(\"\\n---------------------\\n\")\n  }\n}\n\n\nStatistics for Column: mpg \n$mean\n[1] 20.09062\n\n$median\n[1] 19.2\n\n$variance\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$min\n[1] 10.4\n\n$max\n[1] 33.9\n\n\n---------------------\n\nStatistics for Column: cyl \n$mean\n[1] 6.1875\n\n$median\n[1] 6\n\n$variance\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$min\n[1] 4\n\n$max\n[1] 8\n\n\n---------------------\n\nStatistics for Column: disp \n$mean\n[1] 230.7219\n\n$median\n[1] 196.3\n\n$variance\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$min\n[1] 71.1\n\n$max\n[1] 472\n\n\n---------------------\n\nStatistics for Column: hp \n$mean\n[1] 146.6875\n\n$median\n[1] 123\n\n$variance\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$min\n[1] 52\n\n$max\n[1] 335\n\n\n---------------------\n\nStatistics for Column: drat \n$mean\n[1] 3.596563\n\n$median\n[1] 3.695\n\n$variance\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$min\n[1] 2.76\n\n$max\n[1] 4.93\n\n\n---------------------\n\nStatistics for Column: wt \n$mean\n[1] 3.21725\n\n$median\n[1] 3.325\n\n$variance\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$min\n[1] 1.513\n\n$max\n[1] 5.424\n\n\n---------------------\n\nStatistics for Column: qsec \n$mean\n[1] 17.84875\n\n$median\n[1] 17.71\n\n$variance\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$min\n[1] 14.5\n\n$max\n[1] 22.9\n\n\n---------------------\n\nStatistics for Column: vs \n$mean\n[1] 0.4375\n\n$median\n[1] 0\n\n$variance\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: am \n$mean\n[1] 0.40625\n\n$median\n[1] 0\n\n$variance\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: gear \n$mean\n[1] 3.6875\n\n$median\n[1] 4\n\n$variance\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$min\n[1] 3\n\n$max\n[1] 5\n\n\n---------------------\n\nStatistics for Column: carb \n$mean\n[1] 2.8125\n\n$median\n[1] 2\n\n$variance\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$min\n[1] 1\n\n$max\n[1] 8\n\n\n---------------------\n\n\nAs an alternative approach, we can benefit from sapply and apply commands instead of a for loop:\n\nnumeric_stats &lt;- sapply(mtcars[sapply(mtcars, is.numeric)], compute_stats)\nnumeric_stats\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmax      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmax      1         1         5         8       \n\nmatrix_of_mtcars &lt;- as.matrix(mtcars[sapply(mtcars, is.numeric)])\nmatrix_of_statistics &lt;- apply(matrix_of_mtcars, MARGIN = 2, compute_stats)\nmatrix_of_statistics\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$max\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$max\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$max\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$max\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$max\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$max\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$max\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$max\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$max\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$max\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$max\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n\nHandling with a Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a-a-brief-summary-of-the-talk-called-veri-bilimi-ve-endüstri-mühendisliği-üzerine-sohbetler---cem-vardar-erdi-dasdemir",
    "href": "assignments/assignment-1.html#a-a-brief-summary-of-the-talk-called-veri-bilimi-ve-endüstri-mühendisliği-üzerine-sohbetler---cem-vardar-erdi-dasdemir",
    "title": "Assignment 1",
    "section": "(a) A Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”",
    "text": "(a) A Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”\nThe talk is mainly about data science and its relation to industrial engineering according to Mr. Cem Vardar. The talk starts with a brief Introduction where he introduces himself. Mr. Vardar has been an industrial engineer for more than 20 years. After his PhD at Arizona State University, he worked in various tech companies as data scientist and analyst in the USA.\nAs a second section of the talk, he mentioned the concept of Engineering and Problem Solving. According to Mr. Vardar, an engineer solves problem withing the systems by using science and mathematical applications. Here, industrial engineers play a role of problem solver not just of any system but of complex systems. He also emphasizes the importance of initiating the solution to such systems’ problems with a basic approach even though a complex solution will be needed in the end, which is very similar to idea of evolution in his opinion. In this part he states that he supports the phrase “If it works, don’t touch it!” as a response to a student’s question. Later, in Data Science and Industrial Engineering part, he gathers the approaches of data science into sub-groups. These groups mainly use data science as a tool to solve problems, which is a huge plus for a company to analyze and learn.\nIn the fourth section, Carvana and Data Analytics/Science, he explains what the departments related to data do and which tools they are using while doing that in the company named Carvana where he used to work and witnessed its growth thanks to these departments. After that, he mentions the Qualifications of Data Scientists in the business sector and what to do to improve the skills. He basically divides skill into two headings: soft skills and technical skills. Then, he sincerely tells his Recommendations for the ones who are willing to be a successful data scientist as an industrial engineer. In the end, he mentions Reading, Listening and Watching List including some videos and books related to the speech he gave.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#b-exploring-statistical-summaries-with-custom-functions-and-iteration-methods",
    "href": "assignments/assignment-1.html#b-exploring-statistical-summaries-with-custom-functions-and-iteration-methods",
    "title": "Assignment 1",
    "section": "(b) Exploring Statistical Summaries with Custom Functions and Iteration Methods",
    "text": "(b) Exploring Statistical Summaries with Custom Functions and Iteration Methods\n\ndata(mtcars)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nHere, we called “mtcars” dataset.\nThen, the function returning a named list containing the mean, median, variance, IQR, minimum, and maximum of the input would be as follows:\n\ncompute_stats &lt;- function(x){\n  if (!is.numeric(x)){\n    stop(\"Input is not numeric vector.\") #We make sure that the function takes numeric vector as input.\n  }\n  \n  statistics_of_data &lt;- list(\n  mean =  mean(x),\n  median =  median(x),\n  variance = var(x),\n  IQR = IQR(x),\n  min = min(x),\n  max =  max(x)\n  )\n  \n  statistics_of_data\n}\n\nNow, let us apply the function using a for loop:\n\nfor (column_name in names(mtcars)){\n  if(is.numeric(mtcars[[column_name]])){\n    computed_statistics &lt;- compute_stats(mtcars[[column_name]])\n    cat(\"\\nStatistics for Column:\", column_name, \"\\n\")\n    print(computed_statistics)\n    cat(\"\\n---------------------\\n\")\n  }\n}\n\n\nStatistics for Column: mpg \n$mean\n[1] 20.09062\n\n$median\n[1] 19.2\n\n$variance\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$min\n[1] 10.4\n\n$max\n[1] 33.9\n\n\n---------------------\n\nStatistics for Column: cyl \n$mean\n[1] 6.1875\n\n$median\n[1] 6\n\n$variance\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$min\n[1] 4\n\n$max\n[1] 8\n\n\n---------------------\n\nStatistics for Column: disp \n$mean\n[1] 230.7219\n\n$median\n[1] 196.3\n\n$variance\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$min\n[1] 71.1\n\n$max\n[1] 472\n\n\n---------------------\n\nStatistics for Column: hp \n$mean\n[1] 146.6875\n\n$median\n[1] 123\n\n$variance\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$min\n[1] 52\n\n$max\n[1] 335\n\n\n---------------------\n\nStatistics for Column: drat \n$mean\n[1] 3.596563\n\n$median\n[1] 3.695\n\n$variance\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$min\n[1] 2.76\n\n$max\n[1] 4.93\n\n\n---------------------\n\nStatistics for Column: wt \n$mean\n[1] 3.21725\n\n$median\n[1] 3.325\n\n$variance\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$min\n[1] 1.513\n\n$max\n[1] 5.424\n\n\n---------------------\n\nStatistics for Column: qsec \n$mean\n[1] 17.84875\n\n$median\n[1] 17.71\n\n$variance\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$min\n[1] 14.5\n\n$max\n[1] 22.9\n\n\n---------------------\n\nStatistics for Column: vs \n$mean\n[1] 0.4375\n\n$median\n[1] 0\n\n$variance\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: am \n$mean\n[1] 0.40625\n\n$median\n[1] 0\n\n$variance\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: gear \n$mean\n[1] 3.6875\n\n$median\n[1] 4\n\n$variance\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$min\n[1] 3\n\n$max\n[1] 5\n\n\n---------------------\n\nStatistics for Column: carb \n$mean\n[1] 2.8125\n\n$median\n[1] 2\n\n$variance\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$min\n[1] 1\n\n$max\n[1] 8\n\n\n---------------------\n\n\nAs an alternative approach, we can benefit from sapply and apply commands instead of a for loop:\n\nnumeric_stats &lt;- sapply(mtcars[sapply(mtcars, is.numeric)], compute_stats)\nnumeric_stats\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmax      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmax      1         1         5         8       \n\nmatrix_of_mtcars &lt;- as.matrix(mtcars[sapply(mtcars, is.numeric)])\nmatrix_of_statistics &lt;- apply(matrix_of_mtcars, MARGIN = 2, compute_stats)\nmatrix_of_statistics\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$max\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$max\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$max\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$max\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$max\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$max\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$max\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$max\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$max\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$max\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$max\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c-handling-with-a-dataset-na_example",
    "href": "assignments/assignment-1.html#c-handling-with-a-dataset-na_example",
    "title": "Assignment 1",
    "section": "(c) Handling with a Dataset “na_example”",
    "text": "(c) Handling with a Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c-handling-the-dataset-na_example",
    "href": "assignments/assignment-1.html#c-handling-the-dataset-na_example",
    "title": "Assignment 1",
    "section": "(c) Handling the Dataset “na_example”",
    "text": "(c) Handling the Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "project.html#project-overview-and-scope",
    "href": "project.html#project-overview-and-scope",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "",
    "text": "Every country has an inflation which might be either positive or negative. It is stated that positive inflation points to a country’s economic status regarding people’s power to buy and maintain their lives. For the last few years, Turkey has been one of the countries that have suffered from high inflation. Moreover, inflation has been increasing day by day. The inflation word consists of many sub-categories some of which are food, cloth, education, and transportation consumer price index (CPI). This project mainly focuses on the CPI of food and its effect on various groups of people with different total expenditure levels. To analyze the effect and infer, “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025” and “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” data are driven from the website of TUIK, which is short for Turkey Statistics Institute."
  },
  {
    "objectID": "project.html#data",
    "href": "project.html#data",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2. Data",
    "text": "2. Data\n\n\nShow the code\n#install.packages(\"readxl\")\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(simplermarkdown)\nconsumer_price_index_data &lt;- read_xlsx((path = \"consumer_price_index_according_to_groups.xlsx\"), .name_repair = \"unique_quiet\")\ncomparison_of_consumption_types &lt;- read_xlsx(path = \"comparison_of_consumption_types_according_to_expenditure.xlsx\")\n\n\n\n2.1 General Information About Data\nThe data files used for in-depth analysis are driven from the TUIK website. The data named “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” consists of the distribution of consumption expenditure types of different expenditure groups. This data is also referred to as “comparison_of_consumption_types” and “COCT data” throughout the project. There are 5 groups in the columns named “First quantile”, “Second quantile”, “Third quantile”, “Fourth quantile”, and “Last quantile”. Each represents 20% of the people who are subject of the data research in an ascending order of expenditure amount. Namely, the First quantile represents the %20 of the people who spend the least while the Last quantile represents the people who spend the most. There are different expenditure types for the years in the rows. However, there is no data from the years 2020 and 2021 even though the data is named by 2002-2023. Besides, there are two types of expenditure categorization for 2022 with a slight difference in splitting one type into two types. This update of categorization continues in 2023 as well. Consequently, there are 12 different expenditure types for the each year from 2002 to 2022, excluding 2020 and 2021, while there are 14 for the years 2022 and 2023.\n\nAnother data used is named “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025”, which is also referred to as “consumer_price_index_data” and “CPI data” throughout the project. This data shows the consumer price index value of 288 different main and sub-groups of expenditure according to each month of the years from 2005 to 2025. Year and month information is on the rows while the group names form the columns. Only the columns directly related to food are considered as the scope of this project.\n\nThe same years are considered to analyze the relation between CPI of food and its effect on different group of people with different levels of expenditure. Thus, the years between 2005-2023 for both of the data sets are selected as the interval of the project.\n\n\n2.2 Reason of Choice\nThe inflation of a country tells a lot about its economic status. It tells so many things that the food aspect of it might be underestimated. Nevertheless, reaching food has been one of the major concerns of mankind. Over the decades, this concern has become more crucial for Turkish people, especially having low levels of income and belonging to the first and second quantile of expenditure level groups. The data chosen for this project helps us to understand which group of people spend what percentage of their money on food and how this rate changes according to the inflation and consumer index of food. The conclusion of the project might shed light on the facts like a sign of socioeconomic differences among Turkish people. It might also enable society to comprehend the situation of access to quality food according to different expenditure groups.\n\n\n2.3 Preprocessing\nTo begin with, the raw data which are in Excel (.xlsx) format driven from the website of TUIK are browsed as they are. Some of the rows and columns are deleted since they include texts providing information about the data. Later, Turkish headings are removed from each row and column in the files. After a few operations in the Excel format of the files, they become ready to be imported to R. Both data are also saved in RData format to be processed in R.\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row and 8 columns.\n\n\nShow the code\nsave(consumer_price_index_data, file = \"consumer_price_index_data.RData\")\nsave(comparison_of_consumption_types, file = \"comparison_of_consumption_types.RData\")\nload(\"consumer_price_index_data.RData\")\nload(\"comparison_of_consumption_types.RData\")\n\n\n\n\nShow the code\nnrow_CPI &lt;- nrow(consumer_price_index_data)\nncol_CPI &lt;- ncol(consumer_price_index_data)\nnrow_COCT &lt;- nrow(comparison_of_consumption_types)\nncol_COCT &lt;- ncol(comparison_of_consumption_types)\nsummary_of_number_of_row_and_columns &lt;- data.frame(\ndata_set_name = c(\"consumer_price_index_data\",\"comparison_of_consumption_types\"),\nnumber_of_rows = c(nrow_CPI, nrow_COCT),\nnumber_of_columns = c(ncol_CPI, ncol_COCT)\n)\nknitr::kable(summary_of_number_of_row_and_columns, caption = \"Table 1: Dimensions of the Unprocessed Data\")\n\n\n\nTable 1: Dimensions of the Unprocessed Data\n\n\ndata_set_name\nnumber_of_rows\nnumber_of_columns\n\n\n\n\nconsumer_price_index_data\n243\n291\n\n\ncomparison_of_consumption_types\n275\n8\n\n\n\n\n\nDownloadable COCT Data in .RData version\n\nDownloadable CPI Data in .RData version\n\n\n2.3.1 Comparison of Consumption Types (COCT) Data\nIn the data, column names include space (” “) which is not user-friendly in R. Therefore all the spaces is replaced by”_” sign. On the row side, it is observed that a few years are shown with an additional information for some of the years. For instance, year 2007 is represented as “2007(1)”. The reason there are additional value like “(1)” or “(2)” for some of the years is that there is a note about the background of data gathered from those years with the same sign in the very below of the page, which is basically a footnote. In short, all the year value in the first row arranged so that all have only 4 digits. The rows with total consumption expenditure information is also removed since they only have 100% value. Moreover, only the rows having food and non-alcoholic beverages expenditure are kept.  \nAs mentioned in the beginning of General Information About the Data chapter, there are two rows belonging to year 2022 in the data at this point. 2 rows belonging to 2022 is reduced to one row by taking their average. This turned out to be the most complex operation among the ones done in R so far. Finally, the class of Year column is turned into numeric from character and only years between 2005-2023.\n\nThe final look of the data after the operations is as shown below:\n\n\nShow the code\ncomparison_of_consumption_types &lt;- comparison_of_consumption_types %&gt;% \n  rename_with(~ gsub(\" \",\"_\", .x), contains(\" \")) %&gt;% #removing the space \" \" from the columns and replacing them with \"_\"\n  mutate(Year = substr(Year, 1, 4)) %&gt;% #rearranging the Year column\n  filter(Total!=100) %&gt;% #removing the rows having total consumption expenditure information \n  filter(Expenditure_Types==\"Food and non-alcoholic beverages\")\n\n#comparison_of_consumption_types_v1 &lt;- comparison_of_consumption_types %&gt;%\n#  mutate(Year_Expenditure_Type = paste(Year, Expenditure_Types, sep=\"_\")) #mutating a new column including year and expenditure type\n#There is year and expenditure type information for each row. Another column is mutated including year and expenditure type at the same time. There can be unique information column for each row thanks to this operation. This operation is used in the later of this project even though there is no need to such operation. Because we have only one row for each year after removing the unnecessary expenditure types.\n\n\n#Rearranging Year 2022 Columns\nrows_to_merge &lt;- c(19, 20) #the indexes of the rows with year 2022\nmerged_rows &lt;- comparison_of_consumption_types[rows_to_merge, ] #selecting year 2022 rows\nnew_row &lt;- comparison_of_consumption_types[1, ]  # starting to a new data frame\n\nfor (col in names(comparison_of_consumption_types)) { #creating a loop \n  if (is.numeric(comparison_of_consumption_types[[col]])) {\n    new_row[[col]] &lt;- mean(merged_rows[[col]], na.rm = TRUE)\n  } else if (is.character(comparison_of_consumption_types[[col]]) || is.factor(comparison_of_consumption_types[[col]])) {\n    values &lt;- unique(as.character(merged_rows[[col]]))\n    if (length(values) == 1) {\n      new_row[[col]] &lt;- values\n    } else {\n      new_row[[col]] &lt;- NA\n    }\n  } else {\n    new_row[[col]] &lt;- NA\n  }\n}\n\ncomparison_of_consumption_types_updated &lt;- comparison_of_consumption_types[-rows_to_merge, ] #deleting the initial two 2022 rows\n\nnew_position &lt;- nrow(comparison_of_consumption_types_updated)  # position of the generated row\n\n#rearranging the position of the generated row\nCOCT_final &lt;- bind_rows(\n  comparison_of_consumption_types_updated[1:(new_position - 1), ],\n  new_row,\n  comparison_of_consumption_types_updated[new_position:nrow(comparison_of_consumption_types_updated), ]\n)\n\nCOCT_final$Year &lt;- as.numeric(COCT_final$Year) #changing the class of Year column to numeric\n\nCOCT_final &lt;- subset(COCT_final[, -2:-3], Year&gt;=2005) #selecting the years starting from 2005\n\nknitr::kable(COCT_final, caption = \"Table 2: Change of Food Expenditure Percentage According to Quintiles by Years\")\n\n\n\nTable 2: Change of Food Expenditure Percentage According to Quintiles by Years\n\n\n\n\n\n\n\n\n\n\nYear\nFirst_quintile\nSecond_quintile\nThird_quintile\nFourth_quintile\nLast_quintile\n\n\n\n\n2005\n40.63172\n34.88343\n30.04158\n27.07178\n16.74860\n\n\n2006\n39.47612\n33.15669\n29.56472\n26.00507\n17.51656\n\n\n2007\n37.30757\n31.51043\n27.88232\n24.10312\n16.89984\n\n\n2008\n36.44696\n30.21248\n26.02253\n23.32254\n16.28410\n\n\n2009\n34.25025\n29.84173\n26.33608\n24.63697\n16.95754\n\n\n2010\n32.52000\n27.85000\n26.37000\n23.09000\n15.83000\n\n\n2011\n31.69000\n27.42000\n24.82000\n22.39000\n14.61000\n\n\n2012\n31.34000\n26.76000\n24.05000\n21.06000\n13.53000\n\n\n2013\n30.42000\n26.90000\n24.76000\n21.36000\n13.67000\n\n\n2014\n30.12000\n27.41000\n23.86000\n21.61000\n13.60000\n\n\n2015\n31.69000\n27.83000\n25.07000\n21.89000\n13.96000\n\n\n2016\n30.93000\n27.49000\n24.23000\n21.39000\n13.08000\n\n\n2017\n30.73000\n26.48000\n24.88000\n22.11000\n13.35000\n\n\n2018\n30.89000\n27.91000\n25.63000\n22.74000\n13.55000\n\n\n2019\n33.36000\n28.55000\n25.54000\n22.19000\n14.26000\n\n\n2022\n39.24000\n34.28500\n30.34500\n26.48000\n14.19000\n\n\n2023\n39.18000\n31.91000\n27.60000\n23.77000\n12.52000\n\n\n\n\n\n\n\n2.3.2 Consumer Price Index (CPI) Data\nAs an initial sense, there are too many columns as groups of expenditure some of which are not the topic of this project. These columns are removed from the data. However, it is not that easy to obtain all the columns containing food information. After conducting an operation where the aim is selecting the columns containing “food” word, it is observed that there are columns named like “Fish and seafood” and “Other food products…”. What we can deduce is that there might be much more columns related to food than the ones containing “food” word. After a few try, the data appeared to have so many columns including food information like meat, chicken, bread etc.\n\nOn the row side, there is year and month information. An additional row for each year is mutated as the average value of the months of a year. Thanks to this operation, CPI value of food and how it changes can be evaluated not only by months but also by the years. The month column and its relative rows is kept for further analysis.\n\nThe final look of the data after the operations is as shown below:\n\n\nShow the code\n#Generating rows containing average CPI value of each year\nconsumer_price_index_yearly &lt;- consumer_price_index_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %&gt;%\n  mutate(Months = \"Average\") %&gt;%\n  select(Year, Months, everything()) %&gt;%\n  subset(Year&lt;=2023)\n  \n#head(consumer_price_index_data)\n\nconsumer_price_index_food &lt;- consumer_price_index_yearly %&gt;% select(Year | Months | General | contains(\"food\")) #selecting the columns related to food\n\nconsumer_price_index_quality_food &lt;- consumer_price_index_yearly %&gt;% select(Year | Meat | Chicken | `Fish and seafood` | Bread | `Milk, cheese and eggs` | Fruit )\n\nCOCT_final$Year &lt;- as.numeric(COCT_final$Year)\nCPI_food_final &lt;- consumer_price_index_food[, c(1,4)]\nCPI_food_final &lt;- rename(CPI_food_final, \"CPI_of_food\"=\"Food and non-alcoholic beverages\")\n\nknitr::kable(CPI_food_final, caption = \"Table 3: Change of Food CPI by Years\", align=\"l\")\n\n\n\nTable 3: Change of Food CPI by Years\n\n\nYear\nCPI_of_food\n\n\n\n\n2005\n112.0787\n\n\n2006\n122.9450\n\n\n2007\n138.2108\n\n\n2008\n155.8842\n\n\n2009\n168.3875\n\n\n2010\n186.2000\n\n\n2011\n197.8150\n\n\n2012\n214.4567\n\n\n2013\n233.9725\n\n\n2014\n263.4925\n\n\n2015\n292.8617\n\n\n2016\n309.8108\n\n\n2017\n349.1550\n\n\n2018\n411.8717\n\n\n2019\n492.3342\n\n\n2020\n560.5175\n\n\n2021\n696.5883\n\n\n2022\n1293.1833\n\n\n2023\n2144.2950\n\n\n\n\n\nShow the code\n#consumer_price_index_data &lt;- bind_rows(consumer_price_index_data, consumer_price_index_yearly)"
  },
  {
    "objectID": "project.html#analysis",
    "href": "project.html#analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3. Analysis",
    "text": "3. Analysis\nThe analysis part of the project will consist of… There will be plots mainly comparing the CPI of years and months within the years, and comparing food expenditure rates relative to the food CPI values.\n\n3.1 Exploratory Data Analysis\nAfter the preprocessing phase, to look at the graphical reflection of the updated data is a good start for analysis. Initial plotting is done separately to have a general understanding of what the data say.\n\nAs a general conjecture, people with the lowest income level naturally are the ones who spent the least. It is pretty obvious that in Graph 1, the less people have money to spend less the more percent of their money is spent for food. People who spend very less spend at least one third of their money to food. This demonstrates that this part of society do not much have to spend more than the fundamentals like food and housing. Their major concern is to access food.  \nAnother crucial point is as following. Food expenditure level has not been more than 15% of the total expenditure amount for the people belonging the last quintile. However, this much of expenditure might be so close to or even more than the other quintiles. This shows how spending trend changes by the total expenditure amount of people.\n\nCPI of food doubles from 2010 to 2019. However, the percentage of money spent to food between 2010 and 2019 within the quintiles do not change dramatically. Namely, the percentage of food expenditure moves almost steady even though the total amount spent for food and almost everything else increases. This is the case for all the groups.\n\n\nShow the code\nCOCT_long &lt;- COCT_final %&gt;%\n  pivot_longer(cols = -Year,\n               names_to = \"category\",\n               values_to = \"value\")\n\nggplot(COCT_long, aes(x = Year, y = value, color = category, group = category)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  xlab(\"Year\") +\n  ylab(\"Food Expenditure Percentage of Quintiles\") +\n  ggtitle(\"Graph 1: Food Expenditure Percentage by Expenditure Groups Over the Years\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nconsumer_price_index_food %&gt;% \n  filter(Months==\"Average\") %&gt;%\n  ggplot(aes(Year, `Food and non-alcoholic beverages`)) + geom_line(color=\"pink\", linewidth=2) +\n  xlab(\"Year\") +\n  ylab(\"CPI\") +\n  ggtitle(\"Graph ... : Change in Food CPI Over the Years\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\ndf_long &lt;- consumer_price_index_quality_food %&gt;%\n  pivot_longer(cols = -Year, names_to = \"Product\", values_to = \"CPI\")\n\nggplot(df_long, aes(x = Year, y = CPI, color = Product)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Graph ... : Food CPI (2005-2023)\",\n    subtitle = \"Change in High Nutritious Food CPI in Years\",\n    x = \"Year\",\n    y = \"CPI\",\n    color = \"Food\"\n  )\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(df_long, aes(x = Year, y = CPI)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"steelblue\", size = 2) +\n  facet_grid(~ Product, scales = \"free_y\") +\n  theme_minimal(base_size = 14) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    title = \"Graph ... : CPI of High Nutritious Food (2005-2023)\",\n    x = \"Year\",\n    y = \"CPI\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.2 Multiple Linear Regression\nTo analyze the effect of change in CPI of food, multiple linear regression (MLR) technique is used. The reason why MLR is prefered rather than linear regression is that there are different expenditure groups in this study and it is aimed to see the effect of CPI change on these groups seperately.\n\nTwo processed data are merged before starting the analysis. After investigating the output below, the value at the intercept of CPI_of_food and Estimate, which is 9.4, shows that people in the first quintile spend their money with more percentage as CPI increases. On the other hand, other values below 9.4 which are all negative are statistical evidence that other quintiles spend less percentage of their money to food. Different outputs also show that all quintile groups has different characteristics and react differently to change in CPI.\n\n\nShow the code\nmerged_data &lt;- left_join(COCT_final, CPI_food_final, by = \"Year\") #merging two processed data\n\n#Multiple Linear Regression\ndf_long &lt;- pivot_longer(merged_data, cols = starts_with(\"First\"):starts_with(\"Last\"),\n                        names_to = \"Quintile\", values_to = \"Food_Spending\")\n\nmodel_multi &lt;- lm(Food_Spending ~ CPI_of_food + Quintile, data = df_long)\nsummary(model_multi)\n\n\n\nCall:\nlm(formula = Food_Spending ~ CPI_of_food + Quintile, data = df_long)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.866 -1.558 -0.842  1.792  6.789 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              3.374e+01  6.437e-01  52.411  &lt; 2e-16 ***\nCPI_of_food              9.440e-04  5.326e-04   1.772   0.0802 .  \nQuintileFourth_quintile -1.088e+01  8.545e-01 -12.736  &lt; 2e-16 ***\nQuintileLast_quintile   -1.939e+01  8.545e-01 -22.695  &lt; 2e-16 ***\nQuintileSecond_quintile -4.695e+00  8.545e-01  -5.495 4.62e-07 ***\nQuintileThird_quintile  -7.836e+00  8.545e-01  -9.171 4.51e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.491 on 79 degrees of freedom\nMultiple R-squared:  0.8806,    Adjusted R-squared:  0.873 \nF-statistic: 116.5 on 5 and 79 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.3 Trend Analysis and Expenditure Percentage Prediction for CPI Scenarios\n\n\nShow the code\nggplot(df_long, aes(x = CPI_of_food, y = Food_Spending, color = Quintile)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"CPI ve Gıda Harcama Oranı Arasındaki İlişki\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nShow the code\nmodel_A &lt;- lm(First_quintile ~ CPI_of_food, data = merged_data) \npredict(model_A, newdata = data.frame(CPI_of_food = 160))\n\n\n       1 \n33.44224 \n\n\nShow the code\nnew_data &lt;- data.frame(CPI_of_food = c(200, 220, 240))\n\nnew_data$Predicted_Spending &lt;- predict(model_multi, newdata = data.frame(\n  CPI_of_food = c(200, 220, 240),\n  Quintile = rep(\"First_quintile\", 3)\n))\n\n\n\n\n3.4 Elasticity Analysis\nAfter evaluating how food expenditure percentage changes by 1% increase in CPI of food, elasticity coefficient is around 0, slightly more or less. This tells that any expenditure group cannot respond since there is no substitution of food. First quintile seems the one who suffers the most from increse in CPI of food according to the results obtained so far. Nevertheless, the respond to the change is similar for different expenditure levels of groups in Turkey.\n\n\nShow the code\ndf_merged &lt;- merged_data %&gt;%\n  mutate(log_CPI = log(CPI_of_food),\n         log_First = log(First_quintile),\n         log_Second = log(Second_quintile),\n         log_Third = log(Third_quintile),\n         log_Fourth = log(Fourth_quintile),\n         log_Last = log(Last_quintile))\n\nelasticity_model_first_quintile &lt;- lm(log_First ~ log_CPI, data = df_merged)\nelasticity_model_second_quintile &lt;- lm(log_Second ~ log_CPI, data = df_merged)\nelasticity_model_third_quintile &lt;- lm(log_Third ~ log_CPI, data = df_merged)\nelasticity_model_fourth_quintile &lt;- lm(log_Fourth ~ log_CPI, data = df_merged)\nelasticity_model_fifth_quintile &lt;- lm(log_Last ~ log_CPI, data = df_merged)\n\nprint(\"Elasticity of First Quintile\")\n\n\n[1] \"Elasticity of First Quintile\"\n\n\nShow the code\nsummary(elasticity_model_first_quintile)\n\n\n\nCall:\nlm(formula = log_First ~ log_CPI, data = df_merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11876 -0.09418 -0.03791  0.10303  0.19091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.45668    0.19749  17.503 2.16e-11 ***\nlog_CPI      0.01207    0.03470   0.348    0.733    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1099 on 15 degrees of freedom\nMultiple R-squared:  0.008, Adjusted R-squared:  -0.05813 \nF-statistic: 0.121 on 1 and 15 DF,  p-value: 0.7328\n\n\nShow the code\nprint(\"Elasticity of Second Quintile\")\n\n\n[1] \"Elasticity of Second Quintile\"\n\n\nShow the code\nsummary(elasticity_model_second_quintile)\n\n\n\nCall:\nlm(formula = log_Second ~ log_CPI, data = df_merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10395 -0.06537 -0.04744  0.06505  0.18264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.323793   0.167517  19.842 3.54e-12 ***\nlog_CPI     0.009658   0.029432   0.328    0.747    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09318 on 15 degrees of freedom\nMultiple R-squared:  0.007127,  Adjusted R-squared:  -0.05906 \nF-statistic: 0.1077 on 1 and 15 DF,  p-value: 0.7473\n\n\nShow the code\nprint(\"Elasticity of Third Quintile\")\n\n\n[1] \"Elasticity of Third Quintile\"\n\n\nShow the code\nsummary(elasticity_model_third_quintile)\n\n\n\nCall:\nlm(formula = log_Third ~ log_CPI, data = df_merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09380 -0.05402 -0.02554  0.03634  0.14289 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.224864   0.143144  22.529 5.58e-13 ***\nlog_CPI     0.007381   0.025150   0.293    0.773    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07962 on 15 degrees of freedom\nMultiple R-squared:  0.005709,  Adjusted R-squared:  -0.06058 \nF-statistic: 0.08612 on 1 and 15 DF,  p-value: 0.7732\n\n\nShow the code\nprint(\"Elasticity of Fourth Quintile\")\n\n\n[1] \"Elasticity of Fourth Quintile\"\n\n\nShow the code\nsummary(elasticity_model_fourth_quintile)\n\n\n\nCall:\nlm(formula = log_Fourth ~ log_CPI, data = df_merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09700 -0.05713 -0.01771  0.03625  0.15158 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.165317   0.144605  21.889  8.5e-13 ***\nlog_CPI     -0.003901   0.025407  -0.154     0.88    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08044 on 15 degrees of freedom\nMultiple R-squared:  0.001569,  Adjusted R-squared:  -0.06499 \nF-statistic: 0.02357 on 1 and 15 DF,  p-value: 0.88\n\n\nShow the code\nprint(\"Elasticity of Last Quintile\")\n\n\n[1] \"Elasticity of Last Quintile\"\n\n\nShow the code\nsummary(elasticity_model_fifth_quintile)\n\n\n\nCall:\nlm(formula = log_Last ~ log_CPI, data = df_merged)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10581 -0.07313  0.02530  0.04931  0.11167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.21817    0.14099  22.825 4.61e-13 ***\nlog_CPI     -0.09453    0.02477  -3.816  0.00169 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07843 on 15 degrees of freedom\nMultiple R-squared:  0.4926,    Adjusted R-squared:  0.4588 \nF-statistic: 14.56 on 1 and 15 DF,  p-value: 0.001687\n\n\n\n\n3.5 Results"
  },
  {
    "objectID": "project.html#results-and-key-takeaways",
    "href": "project.html#results-and-key-takeaways",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "4. Results and Key Takeaways",
    "text": "4. Results and Key Takeaways"
  }
]