[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! This is Yiğit from EMU660 2024-2025 Spring Course.\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "",
    "text": "Every country has an inflation which might be either positive or negative. It is stated that positive inflation points to a country’s economic status regarding people’s power to buy and maintain their lives. For the last few years, Turkey has been one of the countries that have suffered from high inflation. Moreover, inflation has been increasing day by day. The inflation word consists of many sub-categories some of which are food, cloth, education, and transportation consumer price index (CPI). This project mainly focuses on the CPI of food and its effect on various groups of people with different total expenditure levels. To analyze the effect and infer, “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025” and “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” data are driven from the website of TUIK, which is short for Turkey Statistics Institute."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Inflation of Food and Its Affect on Different Expenditure Groups in Turkey",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nThe data used for the analysis is driven from TUIK website."
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.1 General Information About Data",
    "text": "2.1 General Information About Data\nThe data files used for in-depth analysis are driven from the TUIK website. The data named “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” consists of the distribution of consumption expenditure types of different expenditure groups. This data is also referred to as “comparison_of_consumption_types” and “COCT data” throughout the project. There are 5 groups in the columns named “First quantile”, “Second quantile”, “Third quantile”, “Fourth quantile”, and “Last quantile”. Each represents 20% of the people who are subject of the data research in an ascending order of expenditure amount. Namely, the First quantile represents the %20 of the people who spend the least while the Last quantile represents the people who spend the most. There are different expenditure types for the years in the rows. However, there is no data from the years 2020 and 2021 even though the data is named by 2002-2023. Besides, there are two types of expenditure categorization for 2022 with a slight difference in splitting one type into two types. This update of categorization continues in 2023 as well. Consequently, there are 12 different expenditure types for the years from 2002 to 2022, excluding 2020 and 2021, while there are 14 for the years 2022 and 2023.\n\nAnother data used is named “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025”, which is also referred to as “consumer_price_index_data” and “CPI data” throughout the project. This data shows the consumer price index value of 288 different main and sub-groups of expenditure according to each month of the years from 2005 to 2025. Year and month information is on the rows while the group names form the columns. Only the columns directly related to food are considered as the scope of this project.\n\nIn addition, the same years are considered to analyze the relation between CPI of food and its effect on different group of people with different levels of expenditure. Thus, the years between 2005-2023 for both of the data sets are selected as the interval of the project."
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.2 Reason of Choice",
    "text": "2.2 Reason of Choice\nThe inflation of a country tells a lot about its economic status. It tells so many things that the food aspect of it might be underestimated. Nevertheless, reaching food has been one of the major concerns of mankind. Over the decades, this concern has become more crucial for Turkish people, especially having low levels of income and belonging to the first and second quantile of expenditure level groups. The data chosen for this project helps us to understand which group of people spend what percentage of their money on food and how this rate changes according to the inflation and consumer index of food. The conclusion of the project might shed light on the facts like a sign of socioeconomic differences within the Turkish people. It might also enable society to comprehend the situation of access to quality food according to different expenditure groups."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2.3 Preprocessing",
    "text": "2.3 Preprocessing\nTo begin with, the raw data which are in Excel (.xlsx) format driven from the website of TUIK are browsed as they are. Some of the rows and columns are deleted since they include texts providing information about the data. Later, Turkish headings are removed from each row and column in the files. After a few operations in the Excel format of the files, they become ready to be imported to R. Both data are also saved in RData format to be processed in R.\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row and 8 columns.\n\nsave(consumer_price_index_data, file = \"consumer_price_index_data.RData\")\nsave(comparison_of_consumption_types, file = \"comparison_of_consumption_types.RData\")\nload(\"consumer_price_index_data.RData\")\nload(\"comparison_of_consumption_types.RData\")\n\n\nnrow_CPI &lt;- nrow(consumer_price_index_data)\nncol_CPI &lt;- ncol(consumer_price_index_data)\nnrow_COCT &lt;- nrow(comparison_of_consumption_types)\nncol_COCT &lt;- ncol(comparison_of_consumption_types)\nsummary_of_number_of_row_and_columns &lt;- data.frame(\ndata_set_name = c(\"consumer_price_index_data\",\"comparison_of_consumption_types\"),\nnumber_of_rows = c(nrow_CPI, nrow_COCT),\nnumber_of_column = c(ncol_CPI, ncol_COCT)\n)\nsummary_of_number_of_row_and_columns\n\n                    data_set_name number_of_rows number_of_column\n1       consumer_price_index_data            243              291\n2 comparison_of_consumption_types            275                8\n\nhead(consumer_price_index_data)\n\n# A tibble: 6 × 291\n   Year Months   General Food and non-alcoholic beverag…¹ Alcoholic beverages …²\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;\n1  2005 January     114.                             112.                   122.\n2  2005 February    115.                             113.                   124.\n3  2005 March       115.                             113.                   125.\n4  2005 April       116.                             112.                   125.\n5  2005 May         117.                             112.                   125.\n6  2005 June        117.                             110.                   125.\n# ℹ abbreviated names: ¹​`Food and non-alcoholic beverages`,\n#   ²​`Alcoholic beverages and tobacco`\n# ℹ 286 more variables: `Clothing and footwear` &lt;dbl&gt;,\n#   `Housing, water, electricity, gas and other fuels` &lt;dbl&gt;,\n#   `Furnishings, household equipment, routine maintenance of the house` &lt;dbl&gt;,\n#   Health &lt;dbl&gt;, Transport &lt;dbl&gt;, Communications &lt;dbl&gt;,\n#   `Recreation and culture` &lt;dbl&gt;, Education &lt;dbl&gt;, …\n\nhead(comparison_of_consumption_types)\n\n# A tibble: 6 × 8\n  Year  `Expenditure Types`             Total `First quintile` `Second quintile`\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n1 2002  Total consumption expenditure  100              100               100   \n2 2002  Food and non-alcoholic bevera…  26.7             41.4              37.6 \n3 2002  Alcoholic beverages, cigaratt…   4.06             5.16              4.96\n4 2002  Clothing and footwear            6.27             3.34              4.65\n5 2002  Housing and rent                27.3             33.4              30.7 \n6 2002  Furniture, houses appliances …   7.29             2.45              3.02\n# ℹ 3 more variables: `Third quintile` &lt;dbl&gt;, `Fourth quintile` &lt;dbl&gt;,\n#   `Last quintile` &lt;dbl&gt;\n\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row ans 8 columns."
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3.4 Results",
    "text": "3.4 Results"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignments I am and will be conducting for the Spring 2024-2025 EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nASELSAN A.S, Project Engineer, Sep 2022- (full-time)\nARCELIK A.S, Project Assistant, Jan-Apr 2022 (part-time)\nMercedes-Benz Turk A.S, PEP (Professional Experience Program), Jul-Dec 2021 (part-time)\nBosch Thermotechnic, Logistics Intern, Jun 2021 (internship)\nATM Beyaz Esya Parçalari San. ve Tic. Ltd., Operator, Jun-Aug 2020 (full-time)\nFerhanyildiz Rent a Car, Concierge-Driver, Jun-Aug 2019 (full-time)"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm aaa, position xx, year xxx\nFirm bbb, position yyy, year yyy"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This page consists of the final requirements of Assignment 1. There are three sub-parts of the 3rd task of the assignment, where first two tasks mainly focus on customization of the webpage and publishing the CV.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Education\n\nM.S., Industrial Engineering, Hacettepe University, Turkey, 2024-\nB.S., Industrial Engineering, Ihsan Dogramaci Bilkent University, Turkey, 2017 - 2022\n\n\n\nWork Experience\n\nASELSAN A.S, Project Engineer, Sep 2022- (full-time)\nARCELIK A.S, Project Assistant, Jan-Apr 2022 (part-time)\nMercedes-Benz Turk A.S, PEP (Professional Experience Program), Jul-Dec 2021 (part-time)\nBosch Thermotechnic, Logistics Intern, Jun 2021 (internship)\nATM Beyaz Esya Parçalari San. ve Tic. Ltd., Operator, Jun-Aug 2020 (full-time)\nFerhanyildiz Rent a Car, Concierge-Driver, Jun-Aug 2019 (full-time)\n\n\n\nProjects\n\nDecision Support System for Configuration Management of FNSS (Senior Year Project)\n\n\n\nPublications\n\nProgressing…\n\n\n\nCompetencies\nR, Quarto, Git, Python, Xpress, Primavera P6\n\n\nHobbies\nWorking out, watching-analysing movies, travelling\nCV\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-1.html#a",
    "href": "assignments/assignment-1.html#a",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\n\nA Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”\nThe talk is mainly about data science and its relation to industrial engineering according to Mr. Cem Vardar. The talk starts with a brief Introduction where he introduces himself. Mr. Vardar has been an industrial engineer for more than 20 years. After his PhD at Arizona State University, he worked in various tech companies as data scientist and analyst in the USA.\nAs a second section of the talk, he mentioned the concept of Engineering and Problem Solving. According to Mr. Vardar, an engineer solves problem withing the systems by using science and mathematical applications. Here, industrial engineers play a role of problem solver not just of any system but of complex systems. He also emphasizes the importance of initiating the solution to such systems’ problems with a basic approach even though a complex solution will be needed in the end, which is very similar to idea of evolution in his opinion. In this part he states that he supports the phrase “If it works, don’t touch it!” as a response to a student’s question. Later, in Data Science and Industrial Engineering part, he gathers the approaches of data science into sub-groups. These groups mainly use data science as a tool to solve problems, which is a huge plus for a company to analyze and learn.\nIn the fourth section, Carvana and Data Analytics/Science, he explains what the departments related to data do and which tools they are using while doing that in the company named Carvana where he used to work and witnessed its growth thanks to these departments. After that, he mentions the Qualifications of Data Scientists in the business sector and what to do to improve the skills. He basically divides skill into two headings: soft skills and technical skills. Then, he sincerely tells his Recommendations for the ones who are willing to be a successful data scientist as an industrial engineer. In the end, he mentions Reading, Listening and Watching List including some videos and books related to the speech he gave.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n\nExploring Statistical Summaries with Custom Functions and Iteration Methods\n\ndata(mtcars)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nHere, we called “mtcars” dataset.\nThen, the function returning a named list containing the mean, median, variance, IQR, minimum, and maximum of the input would be as follows:\n\ncompute_stats &lt;- function(x){\n  if (!is.numeric(x)){\n    stop(\"Input is not numeric vector.\") #We make sure that the function takes numeric vector as input.\n  }\n  \n  statistics_of_data &lt;- list(\n  mean =  mean(x),\n  median =  median(x),\n  variance = var(x),\n  IQR = IQR(x),\n  min = min(x),\n  max =  max(x)\n  )\n  \n  statistics_of_data\n}\n\nNow, let us apply the function using a for loop:\n\nfor (column_name in names(mtcars)){\n  if(is.numeric(mtcars[[column_name]])){\n    computed_statistics &lt;- compute_stats(mtcars[[column_name]])\n    cat(\"\\nStatistics for Column:\", column_name, \"\\n\")\n    print(computed_statistics)\n    cat(\"\\n---------------------\\n\")\n  }\n}\n\n\nStatistics for Column: mpg \n$mean\n[1] 20.09062\n\n$median\n[1] 19.2\n\n$variance\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$min\n[1] 10.4\n\n$max\n[1] 33.9\n\n\n---------------------\n\nStatistics for Column: cyl \n$mean\n[1] 6.1875\n\n$median\n[1] 6\n\n$variance\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$min\n[1] 4\n\n$max\n[1] 8\n\n\n---------------------\n\nStatistics for Column: disp \n$mean\n[1] 230.7219\n\n$median\n[1] 196.3\n\n$variance\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$min\n[1] 71.1\n\n$max\n[1] 472\n\n\n---------------------\n\nStatistics for Column: hp \n$mean\n[1] 146.6875\n\n$median\n[1] 123\n\n$variance\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$min\n[1] 52\n\n$max\n[1] 335\n\n\n---------------------\n\nStatistics for Column: drat \n$mean\n[1] 3.596563\n\n$median\n[1] 3.695\n\n$variance\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$min\n[1] 2.76\n\n$max\n[1] 4.93\n\n\n---------------------\n\nStatistics for Column: wt \n$mean\n[1] 3.21725\n\n$median\n[1] 3.325\n\n$variance\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$min\n[1] 1.513\n\n$max\n[1] 5.424\n\n\n---------------------\n\nStatistics for Column: qsec \n$mean\n[1] 17.84875\n\n$median\n[1] 17.71\n\n$variance\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$min\n[1] 14.5\n\n$max\n[1] 22.9\n\n\n---------------------\n\nStatistics for Column: vs \n$mean\n[1] 0.4375\n\n$median\n[1] 0\n\n$variance\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: am \n$mean\n[1] 0.40625\n\n$median\n[1] 0\n\n$variance\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: gear \n$mean\n[1] 3.6875\n\n$median\n[1] 4\n\n$variance\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$min\n[1] 3\n\n$max\n[1] 5\n\n\n---------------------\n\nStatistics for Column: carb \n$mean\n[1] 2.8125\n\n$median\n[1] 2\n\n$variance\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$min\n[1] 1\n\n$max\n[1] 8\n\n\n---------------------\n\n\nAs an alternative approach, we can benefit from sapply and apply commands instead of a for loop:\n\nnumeric_stats &lt;- sapply(mtcars[sapply(mtcars, is.numeric)], compute_stats)\nnumeric_stats\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmax      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmax      1         1         5         8       \n\nmatrix_of_mtcars &lt;- as.matrix(mtcars[sapply(mtcars, is.numeric)])\nmatrix_of_statistics &lt;- apply(matrix_of_mtcars, MARGIN = 2, compute_stats)\nmatrix_of_statistics\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$max\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$max\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$max\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$max\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$max\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$max\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$max\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$max\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$max\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$max\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$max\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n\nHandling with a Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a-a-brief-summary-of-the-talk-called-veri-bilimi-ve-endüstri-mühendisliği-üzerine-sohbetler---cem-vardar-erdi-dasdemir",
    "href": "assignments/assignment-1.html#a-a-brief-summary-of-the-talk-called-veri-bilimi-ve-endüstri-mühendisliği-üzerine-sohbetler---cem-vardar-erdi-dasdemir",
    "title": "Assignment 1",
    "section": "(a) A Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”",
    "text": "(a) A Brief Summary of the Talk Called “Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Cem Vardar & Erdi Dasdemir”\nThe talk is mainly about data science and its relation to industrial engineering according to Mr. Cem Vardar. The talk starts with a brief Introduction where he introduces himself. Mr. Vardar has been an industrial engineer for more than 20 years. After his PhD at Arizona State University, he worked in various tech companies as data scientist and analyst in the USA.\nAs a second section of the talk, he mentioned the concept of Engineering and Problem Solving. According to Mr. Vardar, an engineer solves problem withing the systems by using science and mathematical applications. Here, industrial engineers play a role of problem solver not just of any system but of complex systems. He also emphasizes the importance of initiating the solution to such systems’ problems with a basic approach even though a complex solution will be needed in the end, which is very similar to idea of evolution in his opinion. In this part he states that he supports the phrase “If it works, don’t touch it!” as a response to a student’s question. Later, in Data Science and Industrial Engineering part, he gathers the approaches of data science into sub-groups. These groups mainly use data science as a tool to solve problems, which is a huge plus for a company to analyze and learn.\nIn the fourth section, Carvana and Data Analytics/Science, he explains what the departments related to data do and which tools they are using while doing that in the company named Carvana where he used to work and witnessed its growth thanks to these departments. After that, he mentions the Qualifications of Data Scientists in the business sector and what to do to improve the skills. He basically divides skill into two headings: soft skills and technical skills. Then, he sincerely tells his Recommendations for the ones who are willing to be a successful data scientist as an industrial engineer. In the end, he mentions Reading, Listening and Watching List including some videos and books related to the speech he gave.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#b-exploring-statistical-summaries-with-custom-functions-and-iteration-methods",
    "href": "assignments/assignment-1.html#b-exploring-statistical-summaries-with-custom-functions-and-iteration-methods",
    "title": "Assignment 1",
    "section": "(b) Exploring Statistical Summaries with Custom Functions and Iteration Methods",
    "text": "(b) Exploring Statistical Summaries with Custom Functions and Iteration Methods\n\ndata(mtcars)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nHere, we called “mtcars” dataset.\nThen, the function returning a named list containing the mean, median, variance, IQR, minimum, and maximum of the input would be as follows:\n\ncompute_stats &lt;- function(x){\n  if (!is.numeric(x)){\n    stop(\"Input is not numeric vector.\") #We make sure that the function takes numeric vector as input.\n  }\n  \n  statistics_of_data &lt;- list(\n  mean =  mean(x),\n  median =  median(x),\n  variance = var(x),\n  IQR = IQR(x),\n  min = min(x),\n  max =  max(x)\n  )\n  \n  statistics_of_data\n}\n\nNow, let us apply the function using a for loop:\n\nfor (column_name in names(mtcars)){\n  if(is.numeric(mtcars[[column_name]])){\n    computed_statistics &lt;- compute_stats(mtcars[[column_name]])\n    cat(\"\\nStatistics for Column:\", column_name, \"\\n\")\n    print(computed_statistics)\n    cat(\"\\n---------------------\\n\")\n  }\n}\n\n\nStatistics for Column: mpg \n$mean\n[1] 20.09062\n\n$median\n[1] 19.2\n\n$variance\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$min\n[1] 10.4\n\n$max\n[1] 33.9\n\n\n---------------------\n\nStatistics for Column: cyl \n$mean\n[1] 6.1875\n\n$median\n[1] 6\n\n$variance\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$min\n[1] 4\n\n$max\n[1] 8\n\n\n---------------------\n\nStatistics for Column: disp \n$mean\n[1] 230.7219\n\n$median\n[1] 196.3\n\n$variance\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$min\n[1] 71.1\n\n$max\n[1] 472\n\n\n---------------------\n\nStatistics for Column: hp \n$mean\n[1] 146.6875\n\n$median\n[1] 123\n\n$variance\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$min\n[1] 52\n\n$max\n[1] 335\n\n\n---------------------\n\nStatistics for Column: drat \n$mean\n[1] 3.596563\n\n$median\n[1] 3.695\n\n$variance\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$min\n[1] 2.76\n\n$max\n[1] 4.93\n\n\n---------------------\n\nStatistics for Column: wt \n$mean\n[1] 3.21725\n\n$median\n[1] 3.325\n\n$variance\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$min\n[1] 1.513\n\n$max\n[1] 5.424\n\n\n---------------------\n\nStatistics for Column: qsec \n$mean\n[1] 17.84875\n\n$median\n[1] 17.71\n\n$variance\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$min\n[1] 14.5\n\n$max\n[1] 22.9\n\n\n---------------------\n\nStatistics for Column: vs \n$mean\n[1] 0.4375\n\n$median\n[1] 0\n\n$variance\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: am \n$mean\n[1] 0.40625\n\n$median\n[1] 0\n\n$variance\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\n---------------------\n\nStatistics for Column: gear \n$mean\n[1] 3.6875\n\n$median\n[1] 4\n\n$variance\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$min\n[1] 3\n\n$max\n[1] 5\n\n\n---------------------\n\nStatistics for Column: carb \n$mean\n[1] 2.8125\n\n$median\n[1] 2\n\n$variance\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$min\n[1] 1\n\n$max\n[1] 8\n\n\n---------------------\n\n\nAs an alternative approach, we can benefit from sapply and apply commands instead of a for loop:\n\nnumeric_stats &lt;- sapply(mtcars[sapply(mtcars, is.numeric)], compute_stats)\nnumeric_stats\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmax      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmax      1         1         5         8       \n\nmatrix_of_mtcars &lt;- as.matrix(mtcars[sapply(mtcars, is.numeric)])\nmatrix_of_statistics &lt;- apply(matrix_of_mtcars, MARGIN = 2, compute_stats)\nmatrix_of_statistics\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$max\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$max\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$max\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$max\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$max\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$max\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$max\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$max\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$max\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$max\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$max\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c-handling-with-a-dataset-na_example",
    "href": "assignments/assignment-1.html#c-handling-with-a-dataset-na_example",
    "title": "Assignment 1",
    "section": "(c) Handling with a Dataset “na_example”",
    "text": "(c) Handling with a Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c-handling-the-dataset-na_example",
    "href": "assignments/assignment-1.html#c-handling-the-dataset-na_example",
    "title": "Assignment 1",
    "section": "(c) Handling the Dataset “na_example”",
    "text": "(c) Handling the Dataset “na_example”\nThis time, we will be using dataset “na_example” which is displayed below:\n\nlibrary(dslabs)\ndata(na_example)\nna_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nThis time, we will be using dataset “na_example” which is displayed above. There are some interpretation of the dataset in the following section:\n\nsum(is.na(na_example)) # Count of NA values found within the dataset\n\n[1] 145\n\nwhich(is.na(na_example)) # Index position of NA values in the dataset\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\nmean_of_naexample &lt;- mean(na_example, na.rm = TRUE) # Mean of the dataset\nstd_dev_of_naexample &lt;- sqrt(var(na_example, na.rm = TRUE)) # Standard deviation of the dataset\n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nNow, let us remove the NA’s to find the median of the non-missing values so that we can create the Version 1 of the dataset where all NA values are replaced with the median of the non-missing values.\n\nnonmissing_naexample &lt;- na_example[!is.na(na_example)]\nnonmissing_naexample\n\n  [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 1 4 1 1 2 1 2 2 1 2 5 2 2 3 1 2 4 1 1 1 4 5 2 3\n [38] 4 1 2 4 1 1 2 1 5 1 1 5 1 3 1 4 4 7 3 2 1 4 1 2 2 3 2 1 2 2 4 3 4 2 3 1 3\n [75] 2 1 1 1 3 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3 4 1 1 1 2 4 3 4 3 1 2\n[112] 1 1 5 1 2 1 3 5 3 2 2 3 5 3 1 1 4 2 4 3 3 2 3 2 6 1 1 2 2 1 3 1 1 5 2 4 2\n[149] 5 1 4 3 3 4 3 1 4 1 1 3 1 1 3 5 2 2 2 3 1 2 2 3 2 1 2 1 2 1 1 3 1 2 2 1 3\n[186] 2 2 1 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 1 4 3 1 1 5 2 3 3 2 4 3 2 5 2 3 4 6 2\n[223] 2 2 2 2 3 3 2 2 4 3 1 4 2 2 4 6 2 3 1 2 2 1 1 3 2 3 3 1 1 4 2 1 1 3 2 1 2\n[260] 3 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 2 4 2 1 1 1 3 2 1 1 3 4 1 2 1 1 3 3 1 1\n[297] 3 5 3 2 3 4 1 4 3 1 2 1 2 2 1 2 2 6 1 2 4 5 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4\n[334] 1 3 3 3 2 1 2 1 1 4 2 1 4 4 1 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1\n[371] 1 1 3 2 2 4 4 4 1 1 4 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 4 3 1 3 2 3 1 3 1 4\n[408] 1 1 1 2 4 3 1 2 2 2 3 2 3 1 1 3 2 1 1 2 2 2 2 3 3 1 1 2 1 2 1 1 3 3 1 3 1\n[445] 1 1 1 1 2 5 1 1 2 2 1 1 1 4 1 2 4 1 3 2 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1\n[482] 3 1 3 2 4 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 3 2 2 2 1 5 3 2 3 1 3 1 2 2 2 1 2\n[519] 2 4 6 1 2 1 1 2 2 3 3 2 3 3 4 2 2 4 1 1 2 2 3 1 1 1 3 2 5 7 1 4 3 3 1 1 1\n[556] 1 1 3 2 4 2 2 3 1 4 3 2 2 2 3 2 4 2 2 4 6 3 3 1 4 4 2 1 1 6 3 3 2 1 1 6 1\n[593] 5 1 2 6 2 4 1 3 1 2 1 1 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 2 2\n[630] 2 2 4 5 4 3 3 3 2 4 2 4 2 1 2 4 3 2 2 3 1 3 4 1 2 1 2 3 1 2 1 2 1 2 1 2 2\n[667] 2 2 1 1 3 3 1 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 4 2 1 1 1 3 1 5 2\n[704] 2 4 2 1 3 1 2 1 2 1 2 1 1 3 2 3 2 2 1 4 2 2 4 2 3 1 5 5 2 2 2 2 1 3 1 3 2\n[741] 4 2 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3 2 2 3 2 1 2 4 1 1 1 1 4 3 2 3\n[778] 2 1 3 2 1 1 1 2 2 2 3 3 2 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 4 3 5 1 1 2 2 2\n[815] 2 2 5 2 2 3 1 2 3 1 2 2 3 1 1 2 5 3 5 1 1 4 2 1 3 1 1 2 4 3 3 3 1 1 2 2 1\n[852] 1 2 2 2\n\nmedian_of_nonmissing &lt;- median(nonmissing_naexample)\n\nna_example_Version1 &lt;- ifelse(is.na(na_example), median_of_nonmissing, na_example)\nna_example_Version1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nWe can also come up with Version 2 where all NA values are replaced with the a randomly selected non-missing value by the following operation:\n\nset.seed(123)  # Let us set seed for reproducibility.\nna_example_Version2 &lt;- na_example\nna_example_Version2[is.na(na_example_Version2)] &lt;- sample(nonmissing_naexample, sum(is.na(na_example_Version2)), replace = TRUE)\n\nna_example_Version2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 3 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 3 4 4 7 3 2 3 2 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 1 2 1 5 1 2 1 3 5 3 2 2 1 1 1 3 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 1 1 1 2 2 1 3 1 1 5 2 2 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 3 3 5 2 2 2 3 1 2 2 3 2 1 1 2 5 1 1 3 2 1 1 3 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 4 3 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 1 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 5 2 2 2 1 1 3 2 3 3\n [297] 1 4 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 4 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 4 2 3 1 2 1 1 4 2 1 4 4 4\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 3 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 4 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 4 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 5 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 4 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 1 2 1 4 4 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 3 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 1 1 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 1 2 4 3 3 3\n [741] 2 4 2 4 2 3 3 1 2 1 3 2 4 3 2 2 2 3 1 3 4 1 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 4 2 1 2 4 2 5 1 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 4 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 3 1 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 2 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 2 4 2 6 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nlet us compute the mean and standard deviation of both modified datasets (Version 1 and 2) and compare to the ones of original data.\n\ncat(\"mean of Versiyon 1:\", mean(na_example_Version1), \"\\n\")\n\nmean of Versiyon 1: 2.258 \n\ncat(\"mean of Versiyon 2:\", mean(na_example_Version2), \"\\n\")\n\nmean of Versiyon 2: 2.286 \n\ncat(\"std dev of Version 1:\", sqrt(var(na_example_Version1)), \"\\n\")\n\nstd dev of Version 1: 1.136102 \n\ncat(\"std dev of Version 2:\", sqrt(var(na_example_Version2)), \"\\n\")\n\nstd dev of Version 2: 1.213951 \n\nmean_of_naexample\n\n[1] 2.301754\n\nstd_dev_of_naexample\n\n[1] 1.22338\n\n\nWe can clearly see that the mean values of the modified datasets are less than the original one while on the other hand, standard deviation of the modified datasets are greater than the original one.\nVersion 1 also has greater mean and less standard deviation than Version 1 characteristics. Here, we can conclude that replacing NA values by the median of dataset keeps the data more stable than replacing by a random value.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "project.html#project-overview-and-scope",
    "href": "project.html#project-overview-and-scope",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "",
    "text": "Every country has an inflation which might be either positive or negative. It is stated that positive inflation points to a country’s economic status regarding people’s power to buy and maintain their lives. For the last few years, Turkey has been one of the countries that have suffered from high inflation. Moreover, inflation has been increasing day by day. The inflation word consists of many sub-categories some of which are food, cloth, education, and transportation consumer price index (CPI). This project mainly focuses on the CPI of food and its effect on various groups of people with different total expenditure levels. To analyze the effect and infer, “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025” and “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” data are driven from the website of TUIK, which is short for Turkey Statistics Institute."
  },
  {
    "objectID": "project.html#data",
    "href": "project.html#data",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "2. Data",
    "text": "2. Data\n\n\nShow the code\n#install.packages(\"readxl\")\nlibrary(readxl)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow the code\nlibrary(simplermarkdown)\nconsumer_price_index_data &lt;- read_xlsx((path = \"consumer_price_index_according_to_groups.xlsx\"), .name_repair = \"unique_quiet\")\ncomparison_of_consumption_types &lt;- read_xlsx(path = \"comparison_of_consumption_types_according_to_expenditure.xlsx\")\n\n\n\n2.1 General Information About Data\nThe data files used for in-depth analysis are driven from the TUIK website. The data named “Distribution of household consumption expenditure by quintiles ordered by expenditure, Türkiye, 2002-2023” consists of the distribution of consumption expenditure types of different expenditure groups. This data is also referred to as “comparison_of_consumption_types” and “COCT data” throughout the project. There are 5 groups in the columns named “First quantile”, “Second quantile”, “Third quantile”, “Fourth quantile”, and “Last quantile”. Each represents 20% of the people who are subject of the data research in an ascending order of expenditure amount. Namely, the First quantile represents the %20 of the people who spend the least while the Last quantile represents the people who spend the most. There are different expenditure types for the years in the rows. However, there is no data from the years 2020 and 2021 even though the data is named by 2002-2023. Besides, there are two types of expenditure categorization for 2022 with a slight difference in splitting one type into two types. This update of categorization continues in 2023 as well. Consequently, there are 12 different expenditure types for the each year from 2002 to 2022, excluding 2020 and 2021, while there are 14 for the years 2022 and 2023.\n\nAnother data used is named “Consumer price index (2003=100) according to main and sub-main groups, 2005-2025”, which is also referred to as “consumer_price_index_data” and “CPI data” throughout the project. This data shows the consumer price index value of 288 different main and sub-groups of expenditure according to each month of the years from 2005 to 2025. Year and month information is on the rows while the group names form the columns. Only the columns directly related to food are considered as the scope of this project.\n\nThe same years are considered to analyze the relation between CPI of food and its effect on different group of people with different levels of expenditure. Thus, the years between 2005-2023 for both of the data sets are selected as the interval of the project.\n\n\n2.2 Reason of Choice\nThe inflation of a country tells a lot about its economic status. It tells so many things that the food aspect of it might be underestimated. Nevertheless, reaching food has been one of the major concerns of mankind. Over the decades, this concern has become more crucial for Turkish people, especially having low levels of income and belonging to the first and second quantile of expenditure level groups. The data chosen for this project helps us to understand which group of people spend what percentage of their money on food and how this rate changes according to the inflation and consumer index of food. The conclusion of the project might shed light on the facts like a sign of socioeconomic differences within the Turkish people. It might also enable society to comprehend the situation of access to quality food according to different expenditure groups.\n\n\n2.3 Preprocessing\nTo begin with, the raw data which are in Excel (.xlsx) format driven from the website of TUIK are browsed as they are. Some of the rows and columns are deleted since they include texts providing information about the data. Later, Turkish headings are removed from each row and column in the files. After a few operations in the Excel format of the files, they become ready to be imported to R. Both data are also saved in RData format to be processed in R.\n\nHaving completed the format operations, it is observed that COCT data frame has 243 rows and 291 columns while CPI data frame has 275 row and 8 columns.\n\n\nShow the code\nsave(consumer_price_index_data, file = \"consumer_price_index_data.RData\")\nsave(comparison_of_consumption_types, file = \"comparison_of_consumption_types.RData\")\nload(\"consumer_price_index_data.RData\")\nload(\"comparison_of_consumption_types.RData\")\n\n\n\n\nShow the code\nnrow_CPI &lt;- nrow(consumer_price_index_data)\nncol_CPI &lt;- ncol(consumer_price_index_data)\nnrow_COCT &lt;- nrow(comparison_of_consumption_types)\nncol_COCT &lt;- ncol(comparison_of_consumption_types)\nsummary_of_number_of_row_and_columns &lt;- data.frame(\ndata_set_name = c(\"consumer_price_index_data\",\"comparison_of_consumption_types\"),\nnumber_of_rows = c(nrow_CPI, nrow_COCT),\nnumber_of_columns = c(ncol_CPI, ncol_COCT)\n)\nsummary_of_number_of_row_and_columns\n\n\n                    data_set_name number_of_rows number_of_columns\n1       consumer_price_index_data            243               291\n2 comparison_of_consumption_types            275                 8\n\n\nShow the code\n#head(consumer_price_index_data)\n#head(comparison_of_consumption_types)\n\n\nDownloadable COCT Data in .RData version\n\nDownloadable CPI Data in .RData version\n\n\nFor COCT data;\nIn the data, columnn names include space (” “) which is not user-friendly in R. Therefore all the spaces is replaced by”_” sign. On the row side, it is observed that there are more value for some of the years in addition to the year information. For instance, year 2007 is represented as “2007(1)”. The reason there are additional value like “(1)” or “(2)” for some of the years is that there is a note about the background of data gathered from those years with the same sign in the very below of the page, which is basically a footnote. In short, all the year value in the first row arranged so that all have only 4 digits. The rows with total consumption expenditure information is also removed since they only have 100% value. Moreover, only the rows having food and non-alcoholic beverages expenditure are kept.  \n\nAs mentioned in the beginning of General Information About the Data chapter, there are two row belonging to year 2022 in the data at this point. 2 rows belonging to 2022 is reduced to one row by taking their average. This is the the most complex operation among the ones done in R so far. Finally, the class of Year column is turned into numeric from character.\n\nThe finak look of the data after the operations is as shown below:\n\n\nShow the code\ncomparison_of_consumption_types &lt;- comparison_of_consumption_types %&gt;% \n  rename_with(~ gsub(\" \",\"_\", .x), contains(\" \")) %&gt;% #removing the space \" \" from the columns and replacing them with \"_\"\n  mutate(Year = substr(Year, 1, 4)) %&gt;% #rearranging the Year column\n  filter(Total!=100) %&gt;% #removing the rows having total consumption expenditure information \n  filter(Expenditure_Types==\"Food and non-alcoholic beverages\")\n\n#comparison_of_consumption_types_v1 &lt;- comparison_of_consumption_types %&gt;%\n#  mutate(Year_Expenditure_Type = paste(Year, Expenditure_Types, sep=\"_\")) #mutating a new column including year and expenditure type\n#There is year and expenditure type information for each row. Another column is mutated including year and expenditure type at the same time. There can be unique information column for each row thanks to this operation. This operation is used in the later of this project even though there is no need to such operation. Because we have only one row for each year after removing the unnecessary expenditure types.\n\n\n#Rearranging Year 2022 Columns\nrows_to_merge &lt;- c(19, 20) #the indexes of the rows with year 2022\nmerged_rows &lt;- comparison_of_consumption_types[rows_to_merge, ] #selecting year 2022 rows\nnew_row &lt;- comparison_of_consumption_types[1, ]  # starting to a new data frame\n\nfor (col in names(comparison_of_consumption_types)) { #creating a loop \n  if (is.numeric(comparison_of_consumption_types[[col]])) {\n    new_row[[col]] &lt;- mean(merged_rows[[col]], na.rm = TRUE)\n  } else if (is.character(comparison_of_consumption_types[[col]]) || is.factor(comparison_of_consumption_types[[col]])) {\n    values &lt;- unique(as.character(merged_rows[[col]]))\n    if (length(values) == 1) {\n      new_row[[col]] &lt;- values\n    } else {\n      new_row[[col]] &lt;- NA  # metinler uyuşmuyorsa NA yaz\n    }\n  } else {\n    new_row[[col]] &lt;- NA  # Diğer türler için istersen özelleştir\n  }\n}\n\ncomparison_of_consumption_types_updated &lt;- comparison_of_consumption_types[-rows_to_merge, ] #deleting the initial two 2022 rows\n\n# Yeni satırı sondan bir önceki sıraya ekle\nnew_position &lt;- nrow(comparison_of_consumption_types_updated)  # position of the generated row\n\nCOCT_final &lt;- bind_rows(\n  comparison_of_consumption_types_updated[1:(new_position - 1), ],\n  new_row,\n  comparison_of_consumption_types_updated[new_position:nrow(comparison_of_consumption_types_updated), ]\n)\n\n#summary(COCT_final) #analysing the class of the columns\nCOCT_final$Year &lt;- as.numeric(COCT_final$Year)\nknitr::kable(COCT_final[, -2:-3], caption = \"Table 1: Change of Food Expenditure Percentage According to Quintiles\")\n\n\n\nTable 1: Change of Food Expenditure Percentage According to Quintiles\n\n\n\n\n\n\n\n\n\n\nYear\nFirst_quintile\nSecond_quintile\nThird_quintile\nFourth_quintile\nLast_quintile\n\n\n\n\n2002\n41.41971\n37.57926\n34.86240\n28.96408\n17.78890\n\n\n2003\n43.11486\n38.25576\n34.47400\n30.02119\n18.87998\n\n\n2004\n42.23076\n36.26195\n32.08350\n27.85665\n18.43528\n\n\n2005\n40.63172\n34.88343\n30.04158\n27.07178\n16.74860\n\n\n2006\n39.47612\n33.15669\n29.56472\n26.00507\n17.51656\n\n\n2007\n37.30757\n31.51043\n27.88232\n24.10312\n16.89984\n\n\n2008\n36.44696\n30.21248\n26.02253\n23.32254\n16.28410\n\n\n2009\n34.25025\n29.84173\n26.33608\n24.63697\n16.95754\n\n\n2010\n32.52000\n27.85000\n26.37000\n23.09000\n15.83000\n\n\n2011\n31.69000\n27.42000\n24.82000\n22.39000\n14.61000\n\n\n2012\n31.34000\n26.76000\n24.05000\n21.06000\n13.53000\n\n\n2013\n30.42000\n26.90000\n24.76000\n21.36000\n13.67000\n\n\n2014\n30.12000\n27.41000\n23.86000\n21.61000\n13.60000\n\n\n2015\n31.69000\n27.83000\n25.07000\n21.89000\n13.96000\n\n\n2016\n30.93000\n27.49000\n24.23000\n21.39000\n13.08000\n\n\n2017\n30.73000\n26.48000\n24.88000\n22.11000\n13.35000\n\n\n2018\n30.89000\n27.91000\n25.63000\n22.74000\n13.55000\n\n\n2019\n33.36000\n28.55000\n25.54000\n22.19000\n14.26000\n\n\n2022\n39.24000\n34.28500\n30.34500\n26.48000\n14.19000\n\n\n2023\n39.18000\n31.91000\n27.60000\n23.77000\n12.52000\n\n\n\n\n\n\nFor CPI data;\nThere are too many columns as groups of expenditure some of which are not the topic of this project. These columns will be removed from the data. However it is not that easy to obtain all the columns containing food information. After conducting an operation where the aim is selecting the columns containing “food” word, it is observed that there are columns named like “Fish and seafood” and “Other food products…”. What we can deduce is that there might be much more columns than 6 which all contain “food” word. After a few try, the data appeared to have so many columns including food information like meat, chicken, bread etc. There is even a column for cereal.\n\n\nOn the row side, there is year and month information. An additional row for each year will be mutated as the average value of the months of a year so that a relation with CPI data can be created. The change in the CPI of food will also be evaluated monthly for the years. Therefore, the month column and its relative rows will be kept for further analysis.\n\n\n\nShow the code\n#head(consumer_price_index_data)\nconsumer_price_index_food &lt;- consumer_price_index_data %&gt;% select(contains(\"food\"))\n\n#Generating row containing average CPI value of a year\nyearly_averages &lt;- consumer_price_index_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %&gt;%\n  mutate(Months = \"Average\") %&gt;%\n  select(Year, Months, everything())\n\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(where(is.numeric), mean, na.rm = TRUE)`.\nℹ In group 1: `Year = 2005`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nShow the code\nconsumer_price_index_data &lt;- bind_rows(consumer_price_index_data, yearly_averages)"
  },
  {
    "objectID": "project.html#analysis",
    "href": "project.html#analysis",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "3. Analysis",
    "text": "3. Analysis\nThe analysis part of the project will consist of… There will be plots mainly comparing the CPI of years and months within the years, and comparing food expenditure rates relative to the food CPI values.\n\n3.1 Exploratory Data Analysis\nAfter intense preprocessing phase, is seems logical to look at the graphical reflection of the updated data. Initial plotting is done separately to have general understanding of what the data say.\n\nAs a general conjecture, people with the lowest income level naturally are the ones who spent the least. According to the initial observations people who spent very less spend at least one third of their money to food.\n\n\nShow the code\nCOCT_final %&gt;% ggplot(aes(Year)) + geom_line(aes(x=Year, y=First_quintile, col=1)) + geom_line(aes(x=Year, y=Second_quintile, col=2)) + geom_line(aes(x=Year, y=Third_quintile, col=3)) + geom_line(aes(x=Year, y=Fourth_quintile, col=4)) + geom_line(aes(x=Year, y=Last_quintile, col=5)) +\n  xlab(\"Year\") +\n  ylab(\"Food Expenditure Percentage of Quintiles\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nconsumer_price_index_data %&gt;% \n  filter(Months==\"Average\") %&gt;%\n  ggplot(aes(Year, `Food and non-alcoholic beverages`)) + geom_line() +\n  xlab(\"Year\") +\n  ylab(\"Food Consumer Price Index\")\n\n\n\n\n\n\n\n\n\n\n\n3.2 Trend Analysis\n\n\n3.3 Model Fitting\n\n\n3.4 Results"
  },
  {
    "objectID": "project.html#results-and-key-takeaways",
    "href": "project.html#results-and-key-takeaways",
    "title": "INFLATION OF FOOD AND ITS EFFECT ON DIFFERENT EXPENDITURE GROUPS IN TURKEY",
    "section": "4. Results and Key Takeaways",
    "text": "4. Results and Key Takeaways"
  }
]